{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZYMmNuHQnCPaJbHzRzVB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoftLocked/QuGPT/blob/main/QuGPT_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efhtNWyrVeVR",
        "outputId": "ec7720ca-a29f-4766-fa4d-710d9d51fc00"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit\n",
            "  Downloading qiskit-2.3.0-cp310-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (12 kB)\n",
            "Collecting rustworkx>=0.15.0 (from qiskit)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.12/dist-packages (from qiskit) (1.16.3)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit) (0.3.8)\n",
            "Collecting stevedore>=3.0.0 (from qiskit)\n",
            "  Downloading stevedore-5.6.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit) (4.15.0)\n",
            "Downloading qiskit-2.3.0-cp310-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.6.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stevedore, rustworkx, qiskit\n",
            "Successfully installed qiskit-2.3.0 rustworkx-0.17.1 stevedore-5.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Set\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.quantum_info import Operator"
      ],
      "metadata": {
        "id": "wimdlomeVnpd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token vocabulary (25 tokens)"
      ],
      "metadata": {
        "id": "qqGBrMvBVsAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GateInfo:\n",
        "  '''Describes a token in our vocabulary'''\n",
        "  name: str       # The token's name (e.g. \"h_q0\")\n",
        "  qasm_name: str  # The token's name in qasm 2.0 (e.g. \"h\")\n",
        "  qubits: tuple   # Which qubits it affects (e.g. \"(0,)\" or \"(0, 1)\")\n",
        "  is_t_gate: bool # True for T and Tdg\n",
        "  description: str"
      ],
      "metadata": {
        "id": "PA_86iNYVqen"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary() -> Tuple[Dict[int, GateInfo], Dict[str, int], int, set]:\n",
        "  '''\n",
        "  Build the Clifford+T vocabulary for 2-qubit circuits\n",
        "\n",
        "  Returns:\n",
        "    token_to_gate: dict[int, GateInfo]\n",
        "    gate_to_token: dict[str, int]\n",
        "    vocab_size: int\n",
        "    t_gate_token_ids: set\n",
        "  '''\n",
        "\n",
        "  gates: List[GateInfo] = []\n",
        "\n",
        "  # Special tokens\n",
        "  special = [\n",
        "      (\"<pad>\", \"Padding\"),\n",
        "      (\"<start>\", \"Start of sequence\"),\n",
        "      (\"<end>\", \"End of sequence\")\n",
        "  ]\n",
        "\n",
        "  # 1-qubit Clifford tokens\n",
        "  clifford_1q = [\n",
        "        (\"h\",   \"Hadamard — maps X↔Z, creates superposition\"),\n",
        "        (\"s\",   \"S = √Z — phase gate, quarter-turn around Z\"),\n",
        "        (\"sdg\", \"S† — inverse of S\"),\n",
        "        (\"x\",   \"Pauli X — bit flip\"),\n",
        "        (\"y\",   \"Pauli Y — bit + phase flip\"),\n",
        "        (\"z\",   \"Pauli Z — phase flip\"),\n",
        "  ]\n",
        "\n",
        "  # 2-qubit Clifford tokens\n",
        "  clifford_2q = [\n",
        "        (\"cx\",   \"CNOT — flips target if control is |1⟩\"),\n",
        "        (\"cz\",   \"CZ — applies Z to target if control is |1⟩\"),\n",
        "        (\"swap\", \"SWAP — exchanges the two qubits\"),\n",
        "  ]\n",
        "\n",
        "  # T tokens\n",
        "  t = [\n",
        "      (\"t\", \"T = √S\"),\n",
        "       (\"tdg\", \"T† = inverse of T\")\n",
        "  ]\n",
        "\n",
        "\n",
        "\n",
        "  # Apply special tokens\n",
        "  for name, description in special:\n",
        "    gates.append(GateInfo(name, name, (), False, description))\n",
        "\n",
        "  # Apply 1-qubit Clifford tokens\n",
        "  for gate_name, desc in clifford_1q:\n",
        "        for q in [0, 1]:\n",
        "            gates.append(GateInfo(\n",
        "                name=f\"{gate_name}_q{q}\",\n",
        "                qasm_name=gate_name,\n",
        "                qubits=(q,),\n",
        "                is_t_gate=False,\n",
        "                description=f\"{desc} on q{q}\",\n",
        "            ))\n",
        "\n",
        "  # Apply 2-qubit Clifford tokens\n",
        "  for gate_name, desc in clifford_2q:\n",
        "        for q0, q1 in [(0, 1), (1, 0)]:\n",
        "            gates.append(GateInfo(\n",
        "                name=f\"{gate_name}_q{q0}_q{q1}\",\n",
        "                qasm_name=gate_name,\n",
        "                qubits=(q0, q1),\n",
        "                is_t_gate=False,\n",
        "                description=f\"{desc}: q{q0}→q{q1}\",\n",
        "            ))\n",
        "  # Apply T tokens\n",
        "  for gate_name, desc in t:\n",
        "        for q in [0, 1]:\n",
        "            gates.append(GateInfo(\n",
        "                name=f\"{gate_name}_q{q}\",\n",
        "                qasm_name=gate_name,\n",
        "                qubits=(q,),\n",
        "                is_t_gate=True,\n",
        "                description=f\"{desc} on q{q}\",\n",
        "            ))\n",
        "\n",
        "  # Build lookups\n",
        "  token_to_gate = {i: g for i, g in enumerate(gates)}\n",
        "  gate_to_token = {g.name: i for i, g in enumerate(gates)}\n",
        "  vocab_size = len(gates)\n",
        "\n",
        "  # Identify which token ID's are T gates (to penalize their use)\n",
        "  t_gate_token_ids = {i for i, g in enumerate(gates) if g.is_t_gate}\n",
        "\n",
        "  return token_to_gate, gate_to_token, vocab_size, t_gate_token_ids\n",
        "\n",
        "# Define constants\n",
        "TOKEN_TO_GATE, GATE_TO_TOKEN, VOCAB_SIZE, T_GATE_TOKENS = build_vocabulary()\n",
        "PAD_TOKEN   = GATE_TO_TOKEN['<pad>']\n",
        "START_TOKEN = GATE_TO_TOKEN['<start>']\n",
        "END_TOKEN   = GATE_TO_TOKEN['<end>']\n"
      ],
      "metadata": {
        "id": "AXzhsLSnXF7y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints the full vocabulary for inspection\n",
        "print(f\"Vocabulary: {VOCAB_SIZE} tokens\")\n",
        "print(f\"T-gate token IDs: {T_GATE_TOKENS}\\n\")\n",
        "for i in range(VOCAB_SIZE):\n",
        "    g = TOKEN_TO_GATE[i]\n",
        "    t_marker = \" ← T GATE (expensive!)\" if g.is_t_gate else \"\"\n",
        "    print(f\"  [{i:2d}] {g.name:18s}  qubits={str(g.qubits):10s}  {g.description}{t_marker}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxpMf1AGc9gF",
        "outputId": "2b79318c-ea12-4828-a62c-617af8af3201"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: 25 tokens\n",
            "T-gate token IDs: {24, 21, 22, 23}\n",
            "\n",
            "  [ 0] <pad>               qubits=()          Padding\n",
            "  [ 1] <start>             qubits=()          Start of sequence\n",
            "  [ 2] <end>               qubits=()          End of sequence\n",
            "  [ 3] h_q0                qubits=(0,)        Hadamard — maps X↔Z, creates superposition on q0\n",
            "  [ 4] h_q1                qubits=(1,)        Hadamard — maps X↔Z, creates superposition on q1\n",
            "  [ 5] s_q0                qubits=(0,)        S = √Z — phase gate, quarter-turn around Z on q0\n",
            "  [ 6] s_q1                qubits=(1,)        S = √Z — phase gate, quarter-turn around Z on q1\n",
            "  [ 7] sdg_q0              qubits=(0,)        S† — inverse of S on q0\n",
            "  [ 8] sdg_q1              qubits=(1,)        S† — inverse of S on q1\n",
            "  [ 9] x_q0                qubits=(0,)        Pauli X — bit flip on q0\n",
            "  [10] x_q1                qubits=(1,)        Pauli X — bit flip on q1\n",
            "  [11] y_q0                qubits=(0,)        Pauli Y — bit + phase flip on q0\n",
            "  [12] y_q1                qubits=(1,)        Pauli Y — bit + phase flip on q1\n",
            "  [13] z_q0                qubits=(0,)        Pauli Z — phase flip on q0\n",
            "  [14] z_q1                qubits=(1,)        Pauli Z — phase flip on q1\n",
            "  [15] cx_q0_q1            qubits=(0, 1)      CNOT — flips target if control is |1⟩: q0→q1\n",
            "  [16] cx_q1_q0            qubits=(1, 0)      CNOT — flips target if control is |1⟩: q1→q0\n",
            "  [17] cz_q0_q1            qubits=(0, 1)      CZ — applies Z to target if control is |1⟩: q0→q1\n",
            "  [18] cz_q1_q0            qubits=(1, 0)      CZ — applies Z to target if control is |1⟩: q1→q0\n",
            "  [19] swap_q0_q1          qubits=(0, 1)      SWAP — exchanges the two qubits: q0→q1\n",
            "  [20] swap_q1_q0          qubits=(1, 0)      SWAP — exchanges the two qubits: q1→q0\n",
            "  [21] t_q0                qubits=(0,)        T = √S on q0 ← T GATE (expensive!)\n",
            "  [22] t_q1                qubits=(1,)        T = √S on q1 ← T GATE (expensive!)\n",
            "  [23] tdg_q0              qubits=(0,)        T† = inverse of T on q0 ← T GATE (expensive!)\n",
            "  [24] tdg_q1              qubits=(1,)        T† = inverse of T on q1 ← T GATE (expensive!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input Representation"
      ],
      "metadata": {
        "id": "UaMDgLikfbFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unitary_to_tensor(U: np.ndarray) -> torch.Tensor:\n",
        "  '''Convert 4x4 complex unitary to a flat 32-value real tensor'''\n",
        "  real = torch.tensor(U.real, dtype=torch.float32)\n",
        "  imag = torch.tensor(U.imag, dtype=torch.float32)\n",
        "  return torch.stack([real, imag], dim=-1).flatten()"
      ],
      "metadata": {
        "id": "UfowA42yfZJC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test unitary to tensor converter\n",
        "test_unitary = np.array(\n",
        "    [ [1+1j, 2+2j, 3+3j, 4+4j],\n",
        "      [5+5j, 6+6j, 7+7j, 8+8j],\n",
        "      [9+9j, 10+10j, 11+11j, 12+12j],\n",
        "      [13+13j, 14+14j, 15+15j, 16+16j]\n",
        "     ]\n",
        ")\n",
        "test_tensor = unitary_to_tensor(test_unitary)\n",
        "print(test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj3owQdnfdva",
        "outputId": "db6236f5-0f28-4115-c0cc-56cfebba9ba5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.,  1.,  2.,  2.,  3.,  3.,  4.,  4.,  5.,  5.,  6.,  6.,  7.,  7.,\n",
            "         8.,  8.,  9.,  9., 10., 10., 11., 11., 12., 12., 13., 13., 14., 14.,\n",
            "        15., 15., 16., 16.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder (read the unitary)"
      ],
      "metadata": {
        "id": "YjvN3bVig6Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Each row of the unitary is a token\n",
        "class UnitaryEncoder(nn.Module):\n",
        "  '''\n",
        "  (batch, 32) to (batch, 4, d_model)\n",
        "  '''\n",
        "\n",
        "  def __init__(self, d_model=128, nhead=4, num_layers=4,\n",
        "                 dim_feedforward=512, dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.row_proj = nn.Sequential(\n",
        "      nn.Linear(8, d_model), nn.GELU(),\n",
        "      nn.Linear(d_model, d_model), nn.GELU(),\n",
        "      nn.Linear(d_model, d_model),\n",
        "    )\n",
        "\n",
        "    self.pos_emb = nn.Embedding(4, d_model)\n",
        "\n",
        "    enc_layer = nn.TransformerEncoderLayer(\n",
        "      d_model=d_model,\n",
        "      nhead=nhead,\n",
        "      dim_feedforward=dim_feedforward,\n",
        "      dropout=dropout,\n",
        "      activation='gelu',\n",
        "      batch_first=True,\n",
        "    )\n",
        "\n",
        "    self.transformer = nn.TransformerEncoder(enc_layer, num_layers)\n",
        "\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, unitary_flat: torch.Tensor) -> torch.Tensor:\n",
        "    B = unitary_flat.size(0)\n",
        "    rows = unitary_flat.view(B, 4, 8)\n",
        "    x = self.row_proj(rows) + self.pos_emb(torch.arange(4, device=rows.device))\n",
        "    return self.norm(self.transformer(x))"
      ],
      "metadata": {
        "id": "1USKp1d1hCjQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder (generate circuit tokens)"
      ],
      "metadata": {
        "id": "2S0MQdwWhzhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GateDecoder(nn.Module):\n",
        "    '''\n",
        "    Autoregressive decoder: generates one gate token at a time.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=4,\n",
        "                 dim_feedforward=512, max_seq_len=200, dropout=0.1):\n",
        "      super().__init__()\n",
        "      self.d_model = d_model\n",
        "      self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "      self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "      dec_layer = nn.TransformerDecoderLayer(\n",
        "        d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
        "        dropout=dropout, activation='gelu', batch_first=True,\n",
        "      )\n",
        "      self.transformer = nn.TransformerDecoder(dec_layer, num_layers)\n",
        "      self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "      # Single output head: which gate comes next?\n",
        "      self.gate_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_tokens: torch.Tensor,\n",
        "                encoder_output: torch.Tensor) -> torch.Tensor:\n",
        "      '''\n",
        "      Args:\n",
        "          tgt_tokens:     (batch, seq_len)\n",
        "          encoder_output: (batch, 4, d_model)\n",
        "      Returns:\n",
        "          gate_logits: (batch, seq_len, vocab_size)\n",
        "      '''\n",
        "      S = tgt_tokens.size(1)\n",
        "      device = tgt_tokens.device\n",
        "\n",
        "      x = self.tok_emb(tgt_tokens) * math.sqrt(self.d_model)\n",
        "      x = x + self.pos_emb(torch.arange(S, device=device))\n",
        "\n",
        "      mask = nn.Transformer.generate_square_subsequent_mask(S, device=device)\n",
        "      x = self.norm(self.transformer(tgt=x, memory=encoder_output, tgt_mask=mask))\n",
        "      return self.gate_head(x)"
      ],
      "metadata": {
        "id": "b07k1NyGh3yF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Put together the full model"
      ],
      "metadata": {
        "id": "A-k9P3v-iJUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CliffordTSynthesizer(nn.Module):\n",
        "  '''\n",
        "  Unitary Matrix → Clifford+T Circuit (minimizing T-count).\n",
        "  '''\n",
        "\n",
        "  def __init__(self, d_model=128, nhead=4, num_encoder_layers=4,\n",
        "                num_decoder_layers=5, dim_feedforward=512,\n",
        "                max_seq_len=200, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = UnitaryEncoder(d_model, nhead, num_encoder_layers,\n",
        "                                    dim_feedforward, dropout)\n",
        "    self.decoder = GateDecoder(VOCAB_SIZE, d_model, nhead,\n",
        "                                num_decoder_layers, dim_feedforward,\n",
        "                                max_seq_len, dropout)\n",
        "\n",
        "  def forward(self, unitary_flat: torch.Tensor,\n",
        "              tgt_tokens: torch.Tensor) -> torch.Tensor:\n",
        "    '''\n",
        "    Training forward pass.\n",
        "    Returns gate_logits: (batch, seq_len, vocab_size)\n",
        "    '''\n",
        "\n",
        "    enc_out = self.encoder(unitary_flat)\n",
        "    return self.decoder(tgt_tokens, enc_out)\n"
      ],
      "metadata": {
        "id": "phYEcEAUiOfz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function\n",
        "\n",
        "The loss has two components:\n",
        "\n",
        "1. L = L_gate + λ_T · L_T_count\n",
        "\n",
        "    - L_gate: standard cross-entropy for next-gate prediction.\n",
        "\n",
        "    - L_T_count: a soft penalty on the model's tendency to predict T gates. At each decoding position, we look at the probability the model assigns to T/Tdg tokens. Summing these across the sequence gives the \"expected T-count\", which we penalize.\n",
        "\n",
        "        - This is differentiable (unlike counting discrete tokens), so the model can learn to avoid T gates through gradient descent.\n",
        "\n",
        "2. The λ_T weight controls the tradeoff:\n",
        "    - λ_T = 0:    pure accuracy, no T minimization\n",
        "    - λ_T = 0.1:  mild preference for fewer T gates\n",
        "    - λ_T = 0.5+: aggressive T reduction (may hurt accuracy)\n",
        "\n",
        "During training, we anneal λ_T from 0 → target value so the model\n",
        "first learns correct circuits, then learns to compress T-count."
      ],
      "metadata": {
        "id": "Sfo7sUI2im9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CliffordTLoss(nn.Module):\n",
        "    '''\n",
        "    Combined loss: cross-entropy + T-count penalty.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, t_gate_token_ids: Set[int], t_penalty_weight: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "        self.t_gate_token_ids = sorted(t_gate_token_ids)\n",
        "        self.t_penalty_weight = t_penalty_weight\n",
        "\n",
        "    def forward(self, gate_logits: torch.Tensor,\n",
        "                target_tokens: torch.Tensor,\n",
        "                t_penalty_scale: float = 1.0) -> Tuple[torch.Tensor, ...]:\n",
        "        '''\n",
        "        Args:\n",
        "            gate_logits:     (batch, seq_len, vocab_size)\n",
        "            target_tokens:   (batch, seq_len)\n",
        "            t_penalty_scale: multiplier for annealing (0→1 during training)\n",
        "\n",
        "        Returns:\n",
        "            total_loss, gate_loss, t_penalty\n",
        "        '''\n",
        "        B, S, V = gate_logits.shape\n",
        "\n",
        "        # Standard cross-entropy: did we predict the right gate?\n",
        "        gate_loss = self.ce_loss(\n",
        "            gate_logits.reshape(-1, V), target_tokens.reshape(-1)\n",
        "        )\n",
        "\n",
        "        # T-count penalty: discourage predicting T gates\n",
        "        # Get probability distribution at each position\n",
        "        probs = F.softmax(gate_logits, dim=-1)  # (B, S, V)\n",
        "\n",
        "        # Sum probabilities assigned to T and Tdg tokens\n",
        "        t_probs = probs[:, :, self.t_gate_token_ids]  # (B, S, num_t_tokens)\n",
        "        expected_t_per_position = t_probs.sum(dim=-1)  # (B, S)\n",
        "\n",
        "        # Mask out padding positions\n",
        "        pad_mask = (target_tokens != PAD_TOKEN).float()  # (B, S)\n",
        "        expected_t_count = (expected_t_per_position * pad_mask).sum(dim=-1)  # (B,)\n",
        "\n",
        "        # Average over batch\n",
        "        t_penalty = expected_t_count.mean()\n",
        "\n",
        "        # Combine with annealing\n",
        "        effective_t_weight = self.t_penalty_weight * t_penalty_scale\n",
        "        total_loss = gate_loss + effective_t_weight * t_penalty\n",
        "\n",
        "        return total_loss, gate_loss, t_penalty\n"
      ],
      "metadata": {
        "id": "GsxRYKiAiqHE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training data generation\n",
        "\n",
        "Generate random Clifford+T circuits, compute their unitaries, and use the unitary and gate sequence for training\n",
        "\n",
        "Data is biased for circuits to have varied T-gate counts."
      ],
      "metadata": {
        "id": "ogozmRQqkHW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_gate(qc: QuantumCircuit, gate_name: str, qubits: tuple):\n",
        "    '''\n",
        "    Apply a gate to a Qiskit circuit. No parameters needed!\n",
        "    '''\n",
        "    q = list(qubits)\n",
        "    gate_map = {\n",
        "        \"h\": lambda: qc.h(q[0]),       \"s\": lambda: qc.s(q[0]),\n",
        "        \"sdg\": lambda: qc.sdg(q[0]),   \"x\": lambda: qc.x(q[0]),\n",
        "        \"y\": lambda: qc.y(q[0]),       \"z\": lambda: qc.z(q[0]),\n",
        "        \"t\": lambda: qc.t(q[0]),       \"tdg\": lambda: qc.tdg(q[0]),\n",
        "        \"cx\": lambda: qc.cx(q[0], q[1]),\n",
        "        \"cz\": lambda: qc.cz(q[0], q[1]),\n",
        "        \"swap\": lambda: qc.swap(q[0], q[1]),\n",
        "    }\n",
        "    gate_map[gate_name]()"
      ],
      "metadata": {
        "id": "9y9akaEVk1Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gate names grouped by type for weighted sampling\n",
        "CLIFFORD_1Q_TOKENS = [f\"{g}_q{q}\" for g in [\"h\",\"s\",\"sdg\",\"x\",\"y\",\"z\"] for q in [0,1]]\n",
        "T_GATE_TOKENS_LIST = [f\"{g}_q{q}\" for g in [\"t\",\"tdg\"] for q in [0,1]]\n",
        "CLIFFORD_2Q_TOKENS = [f\"{g}_q{q0}_q{q1}\" for g in [\"cx\",\"cz\",\"swap\"]\n",
        "                      for q0, q1 in [(0,1),(1,0)]]\n",
        "ALL_GATE_TOKENS = CLIFFORD_1Q_TOKENS + T_GATE_TOKENS_LIST + CLIFFORD_2Q_TOKENS"
      ],
      "metadata": {
        "id": "90gKOCNClTwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_sample(max_gates: int = 20,\n",
        "                           max_t_count: int = 6) -> Tuple[np.ndarray, List[int]]:\n",
        "    '''\n",
        "    Generate a random Clifford+T circuit with controlled T-count.\n",
        "\n",
        "    The max_t_count parameter limits how many T/Tdg gates appear,\n",
        "    biasing the dataset toward T-efficient circuits.\n",
        "\n",
        "    Returns:\n",
        "        unitary:   4×4 complex numpy array\n",
        "        token_ids: list of ints [START, gate1, ..., gateN, END]\n",
        "    '''\n",
        "\n",
        "    qc = QuantumCircuit(2)\n",
        "    token_ids = [START_TOKEN]\n",
        "    t_count = 0\n",
        "    num_gates = random.randint(1, max_gates)\n",
        "\n",
        "    for _ in range(num_gates):\n",
        "        # Weighted sampling: Clifford gates are free, T gates are expensive\n",
        "        # We allow T gates only if we haven't hit the limit\n",
        "        if t_count < max_t_count and random.random() < 0.2:\n",
        "            # 20% chance of T gate (if under limit)\n",
        "            gate_name = random.choice(T_GATE_TOKENS_LIST)\n",
        "            t_count += 1\n",
        "        elif random.random() < 0.3:\n",
        "            # 30% chance of 2-qubit Clifford\n",
        "            gate_name = random.choice(CLIFFORD_2Q_TOKENS)\n",
        "        else:\n",
        "            # 50% chance of 1-qubit Clifford\n",
        "            gate_name = random.choice(CLIFFORD_1Q_TOKENS)\n",
        "\n",
        "        token_id = GATE_TO_TOKEN[gate_name]\n",
        "        gate_info = TOKEN_TO_GATE[token_id]\n",
        "        apply_gate(qc, gate_info.qasm_name, gate_info.qubits)\n",
        "        token_ids.append(token_id)\n",
        "\n",
        "    token_ids.append(END_TOKEN)\n",
        "    unitary = Operator(qc).data\n",
        "    return unitary, token_ids"
      ],
      "metadata": {
        "id": "8A-GiJ5jlY42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset_with_varied_t_counts(num_samples: int = 10000\n",
        "                                           ) -> List[Tuple[np.ndarray, List[int]]]:\n",
        "    '''\n",
        "    Generate training data with a mix of T-counts to teach the model\n",
        "    that low-T solutions exist.\n",
        "\n",
        "    Distribution:\n",
        "      - 30% pure Clifford (T-count = 0)\n",
        "      - 30% low T-count (1-2)\n",
        "      - 25% medium T-count (3-4)\n",
        "      - 15% higher T-count (5-8)\n",
        "    '''\n",
        "\n",
        "    data = []\n",
        "    print(f\"Generating {num_samples} training samples...\")\n",
        "    for i in range(num_samples):\n",
        "        if (i + 1) % 2000 == 0:\n",
        "            print(f\"  {i+1}/{num_samples}\")\n",
        "\n",
        "        r = random.random()\n",
        "        if r < 0.30:\n",
        "            max_t, max_g = 0, 15     # Pure Clifford\n",
        "        elif r < 0.60:\n",
        "            max_t, max_g = 2, 15     # Low T\n",
        "        elif r < 0.85:\n",
        "            max_t, max_g = 4, 20     # Medium T\n",
        "        else:\n",
        "            max_t, max_g = 8, 25     # Higher T\n",
        "\n",
        "        U, tokens = generate_random_sample(max_gates=max_g, max_t_count=max_t)\n",
        "        data.append((U, tokens))\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "uS8d1OPcllBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}